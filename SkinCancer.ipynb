{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/datasets/rajatvisitme/skin-cancer-isic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install tensorflow==2.15.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical  # Updated import\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(data_dir):\n",
    "    data = [\n",
    "        {\"image_path\": os.path.join(data_dir, dir_name, fname), \"label\": label}\n",
    "        for label, dir_name in enumerate(os.listdir(data_dir))\n",
    "        for fname in os.listdir(os.path.join(data_dir, dir_name))\n",
    "    ]\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "train_dir = r'C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Projects\\Skin Cancer Project\\Skin cancer ISIC Dataset\\Train'\n",
    "test_dir = r'C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Projects\\Skin Cancer Project\\Skin cancer ISIC Dataset\\Test'\n",
    "\n",
    "# Create combined dataframe\n",
    "df = pd.concat([create_dataframe(train_dir), create_dataframe(test_dir)], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1312</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1218</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1131</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>675</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>724</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             image_path  label\n",
       "495   C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      2\n",
       "94    C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      0\n",
       "41    C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      0\n",
       "976   C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      3\n",
       "1312  C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      4\n",
       "1218  C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      4\n",
       "1131  C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      4\n",
       "675   C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      3\n",
       "724   C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      3\n",
       "981   C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Project...      3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'actinic keratosis',\n",
       " 1: 'basal cell carcinoma',\n",
       " 2: 'dermatofibroma',\n",
       " 3: 'melanoma',\n",
       " 4: 'nevus',\n",
       " 5: 'vascular lesion'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get list of directories and create label_map\n",
    "label_map = {i: label for i, label in enumerate(os.listdir(train_dir))}\n",
    "num_classes = len(label_map)\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_images_per_class = 2000\n",
    "\n",
    "# Limit each class to max_images_per_class samples\n",
    "df = df.groupby(\"label\").head(max_images_per_class).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "print(gpus)\n",
    "try:\n",
    "    for gpu in gpus:\n",
    "        tf.config.experimental.set_memory_growth(gpu, True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "# Get the number of CPU cores\n",
    "max_workers = multiprocessing.cpu_count()\n",
    "print(max_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "# Function to resize image arrays\n",
    "def resize_image_array(image_path):\n",
    "    return np.asarray(Image.open(image_path).resize((100, 75)))\n",
    "\n",
    "# Parallelize resizing process\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "    df['image'] = list(executor.map(resize_image_array, df['image_path']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ImageDataGenerator with transformations\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.5,\n",
    "    height_shift_range=0.25,\n",
    "    shear_range=0.25,\n",
    "    zoom_range=0.25,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize augmented DataFrame\n",
    "augmented_df = pd.DataFrame(columns=['image_path', 'label', 'image'])\n",
    "\n",
    "for class_label in df['label'].unique():\n",
    "    # Get image arrays for the current class\n",
    "    class_images = df[df['label'] == class_label]\n",
    "    image_arrays = class_images['image'].values\n",
    "    num_images_needed = max_images_per_class - len(image_arrays)\n",
    "    \n",
    "    # Add original images to augmented DataFrame\n",
    "    augmented_df = pd.concat([augmented_df, class_images], ignore_index=True)\n",
    "\n",
    "    if num_images_needed > 0:\n",
    "        selected_images = np.random.choice(image_arrays, size=num_images_needed, replace=True)\n",
    "        \n",
    "        # Augment and add new images\n",
    "        for img_array in selected_images:\n",
    "            image_tensor = np.expand_dims(img_array, axis=0)\n",
    "            augmented_images = datagen.flow(image_tensor, batch_size=1)\n",
    "            \n",
    "            for _ in range(1):  # Generate one augmented image at a time\n",
    "                augmented_img = augmented_images.__next__()[0].astype('uint8')\n",
    "                new_row = pd.DataFrame([{'image_path': None, 'label': class_label, 'image': augmented_img}])\n",
    "                augmented_df = pd.concat([augmented_df, new_row], ignore_index=True)\n",
    "\n",
    "# Limit the size of each class\n",
    "df = augmented_df.groupby('label').head(max_images_per_class).sample(frac=1, random_state=42).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary\n",
      "------------------------------------------------------------\n",
      "Class Label     Class Name                     Count     \n",
      "------------------------------------------------------------\n",
      "0               actinic keratosis              2000      \n",
      "1               basal cell carcinoma           2000      \n",
      "2               dermatofibroma                 2000      \n",
      "3               melanoma                       2000      \n",
      "4               nevus                          2000      \n",
      "5               vascular lesion                2000      \n",
      "------------------------------------------------------------\n",
      "Total                                         12000     \n"
     ]
    }
   ],
   "source": [
    "# Count the number of images in each class\n",
    "class_counts = df['label'].value_counts().sort_index()\n",
    "\n",
    "# Print the dataset summary\n",
    "print(\"Dataset Summary\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Class Label':<15} {'Class Name':<30} {'Count':<10}\")\n",
    "print(\"-\" * 60)\n",
    "for label, name in label_map.items():\n",
    "    print(f\"{label:<15} {name:<30} {class_counts[label]:<10}\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"{'Total':<45} {class_counts.sum():<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns=['label','image_path'],axis=1)\n",
    "y = df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert images to numpy arrays\n",
    "X_train = np.asarray(X_train['image'].tolist())\n",
    "X_test = np.asarray(X_test['image'].tolist())\n",
    "\n",
    "# Normalize images\n",
    "X_train_mean, X_train_std = X_train.mean(), X_train.std()\n",
    "X_test_mean, X_test_std = X_test.mean(), X_test.std()\n",
    "\n",
    "X_train = (X_train - X_train_mean) / X_train_std\n",
    "X_test = (X_test - X_test_mean) / X_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = to_categorical(y_train, num_classes=num_classes)\n",
    "y_test = to_categorical(y_test, num_classes=num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and validation sets\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X_train, y_train, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape images to 3D (height=75px, width=100px, channels=3)\n",
    "X_train = X_train.reshape(X_train.shape[0], 75, 100, 3)\n",
    "X_test = X_test.reshape(X_test.shape[0], 75, 100, 3)\n",
    "X_validate = X_validate.reshape(X_validate.shape[0], 75, 100, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.astype(int)\n",
    "y_validate = y_validate.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary\n",
      "------------------------------------------------------------------------------------------\n",
      "Class Label     Class Name                     Train      Validation   Test       Total     \n",
      "------------------------------------------------------------------------------------------\n",
      "0               actinic keratosis              1280       317          403        2000      \n",
      "1               basal cell carcinoma           1275       332          393        2000      \n",
      "2               dermatofibroma                 1242       339          419        2000      \n",
      "3               melanoma                       1284       301          415        2000      \n",
      "4               nevus                          1297       322          381        2000      \n",
      "5               vascular lesion                1302       309          389        2000      \n",
      "------------------------------------------------------------------------------------------\n",
      "Total                                          7680       1920         2400       12000     \n"
     ]
    }
   ],
   "source": [
    "# Count the number of images per class\n",
    "train_counts = np.sum(y_train, axis=0)\n",
    "val_counts = np.sum(y_validate, axis=0)\n",
    "test_counts = np.sum(y_test, axis=0)\n",
    "\n",
    "# Print dataset summary\n",
    "print(\"Dataset Summary\")\n",
    "print(\"-\" * 90)\n",
    "print(f\"{'Class Label':<15} {'Class Name':<30} {'Train':<10} {'Validation':<12} {'Test':<10} {'Total':<10}\")\n",
    "print(\"-\" * 90)\n",
    "for label, name in label_map.items():\n",
    "    train_num = int(train_counts[label])\n",
    "    val_num = int(val_counts[label])\n",
    "    test_num = int(test_counts[label])\n",
    "    total_num = train_num + val_num + test_num\n",
    "    print(f\"{label:<15} {name:<30} {train_num:<10} {val_num:<12} {test_num:<10} {total_num:<10}\")\n",
    "print(\"-\" * 90)\n",
    "total_images = len(y_train) + len(y_validate) + len(y_test)\n",
    "print(f\"{'Total':<46} {len(y_train):<10} {len(y_validate):<12} {len(y_test):<10} {total_images:<10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(75, 100, 3)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_shape = df['image'][0].shape\n",
    "input_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.resnet import preprocess_input as resnet_preprocess_input\n",
    "from tensorflow.keras.applications import DenseNet201\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout,GlobalAveragePooling2D\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ densenet201 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1920</span>)     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">18,321,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11520</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11520</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)            │     <span style=\"color: #00af00; text-decoration-color: #00af00\">5,898,752</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,078</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ densenet201 (\u001b[38;5;33mFunctional\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1920\u001b[0m)     │    \u001b[38;5;34m18,321,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11520\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m11520\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)            │     \u001b[38;5;34m5,898,752\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)              │         \u001b[38;5;34m3,078\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">24,223,814</span> (92.41 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m24,223,814\u001b[0m (92.41 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,994,758</span> (91.53 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m23,994,758\u001b[0m (91.53 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">229,056</span> (894.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m229,056\u001b[0m (894.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define the DenseNet201 model with custom layers\n",
    "model = Sequential([\n",
    "    DenseNet201(include_top=False, weights='imagenet', input_shape=input_shape),\n",
    "    Flatten(),\n",
    "    Dropout(0.5),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import  ReduceLROnPlateau\n",
    "\n",
    "# Compile the model\n",
    "opt = SGD(learning_rate=0.0001, momentum=0.9)\n",
    "model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "\n",
    "# Set learning rate annealer\n",
    "learning_rate_reduction = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    patience=3,\n",
    "    verbose=1,\n",
    "    factor=0.5,\n",
    "    min_lr=0.00001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m745s\u001b[0m 3s/step - accuracy: 0.3003 - loss: 2.0112 - val_accuracy: 0.6260 - val_loss: 1.0195 - learning_rate: 1.0000e-04\n",
      "Epoch 2/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m661s\u001b[0m 3s/step - accuracy: 0.6026 - loss: 1.0569 - val_accuracy: 0.7016 - val_loss: 0.8061 - learning_rate: 1.0000e-04\n",
      "Epoch 3/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m668s\u001b[0m 3s/step - accuracy: 0.6871 - loss: 0.8427 - val_accuracy: 0.7380 - val_loss: 0.7127 - learning_rate: 1.0000e-04\n",
      "Epoch 4/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m686s\u001b[0m 3s/step - accuracy: 0.7641 - loss: 0.6543 - val_accuracy: 0.7646 - val_loss: 0.6399 - learning_rate: 1.0000e-04\n",
      "Epoch 5/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 3s/step - accuracy: 0.8030 - loss: 0.5384 - val_accuracy: 0.7891 - val_loss: 0.5920 - learning_rate: 1.0000e-04\n",
      "Epoch 6/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m662s\u001b[0m 3s/step - accuracy: 0.8356 - loss: 0.4655 - val_accuracy: 0.7906 - val_loss: 0.5735 - learning_rate: 1.0000e-04\n",
      "Epoch 7/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m673s\u001b[0m 3s/step - accuracy: 0.8687 - loss: 0.3601 - val_accuracy: 0.8115 - val_loss: 0.5303 - learning_rate: 1.0000e-04\n",
      "Epoch 8/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m665s\u001b[0m 3s/step - accuracy: 0.8891 - loss: 0.3181 - val_accuracy: 0.8224 - val_loss: 0.5184 - learning_rate: 1.0000e-04\n",
      "Epoch 9/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m671s\u001b[0m 3s/step - accuracy: 0.8991 - loss: 0.2793 - val_accuracy: 0.8266 - val_loss: 0.5102 - learning_rate: 1.0000e-04\n",
      "Epoch 10/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m669s\u001b[0m 3s/step - accuracy: 0.9153 - loss: 0.2339 - val_accuracy: 0.8359 - val_loss: 0.5171 - learning_rate: 1.0000e-04\n",
      "Epoch 11/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m682s\u001b[0m 3s/step - accuracy: 0.9293 - loss: 0.2065 - val_accuracy: 0.8385 - val_loss: 0.5040 - learning_rate: 1.0000e-04\n",
      "Epoch 12/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 3s/step - accuracy: 0.9430 - loss: 0.1713 - val_accuracy: 0.8380 - val_loss: 0.5150 - learning_rate: 1.0000e-04\n",
      "Epoch 13/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m678s\u001b[0m 3s/step - accuracy: 0.9473 - loss: 0.1501 - val_accuracy: 0.8427 - val_loss: 0.5085 - learning_rate: 1.0000e-04\n",
      "Epoch 14/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m698s\u001b[0m 3s/step - accuracy: 0.9533 - loss: 0.1303 - val_accuracy: 0.8438 - val_loss: 0.5233 - learning_rate: 1.0000e-04\n",
      "Epoch 15/15\n",
      "\u001b[1m240/240\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m681s\u001b[0m 3s/step - accuracy: 0.9603 - loss: 0.1132 - val_accuracy: 0.8516 - val_loss: 0.5230 - learning_rate: 1.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 10h 48min 11s\n",
      "Wall time: 2h 49min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Fit the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=15, batch_size=32,\n",
    "    validation_data=(X_validate, y_validate),\n",
    "    callbacks=[learning_rate_reduction]\n",
    ")\n",
    "\n",
    "model.save('skin_disease_6_classes_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = load_model('skin_disease_6_classes_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 312ms/step\n",
      "Predicted Class: nevus\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the image\n",
    "image_path=r'C:\\Users\\Pc\\Documents\\OneDrive\\Desktop\\Projects\\Skin Cancer Project\\Skin cancer ISIC Dataset\\Train\\actinic keratosis\\ISIC_0026984.jpg'\n",
    "\n",
    "# Load the image and preprocess it\n",
    "img = image.load_img(image_path, target_size=(75, 100))  # Correct order: height=75px, width=100px\n",
    "img_array = image.img_to_array(img)  # Convert image to array\n",
    "img_array = np.expand_dims(img_array, axis=0)  # Add batch dimension\n",
    "\n",
    "# Normalize the image as per the training data preprocessing\n",
    "img_array = (img_array - np.mean(img_array)) / np.std(img_array)\n",
    "\n",
    "\n",
    "# Predict the class\n",
    "predictions = model.predict(img_array)\n",
    "predicted_class = np.argmax(predictions, axis=1)\n",
    "\n",
    "# Map the predicted class index to the class label\n",
    "predicted_label = label_map[predicted_class[0]]\n",
    "\n",
    "print(f\"Predicted Class: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
